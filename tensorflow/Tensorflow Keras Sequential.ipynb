{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "[[0.74737763]\n",
      " [0.61622047]\n",
      " [0.93466686]\n",
      " [0.25471864]\n",
      " [0.04724408]\n",
      " [0.42497113]\n",
      " [0.23897638]\n",
      " [0.23473852]\n",
      " [0.30790719]\n",
      " [0.17497578]]\n"
     ]
    }
   ],
   "source": [
    "# create a random sample (about 1.000) of integers\n",
    "train_data = np.random.random_sample((1000, 1))\n",
    "\n",
    "# label the data such that it's 1 if the value is greater than 0.5, 0 otherwise:\n",
    "labels = []\n",
    "for value in train_data:\n",
    "  labels.append(1 if (value > 0.5) else 0)\n",
    "print(labels)\n",
    "print(train_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 429us/step - loss: 0.6780 - acc: 0.7170\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.6564 - acc: 0.6410\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.6355 - acc: 0.6980\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.6175 - acc: 0.6900\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.6004 - acc: 0.7170\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.5817 - acc: 0.7590\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.5618 - acc: 0.7960\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.5409 - acc: 0.8270\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.5196 - acc: 0.8550\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.4970 - acc: 0.8700\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.4740 - acc: 0.8770\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.4514 - acc: 0.8820\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.4281 - acc: 0.9120\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.4060 - acc: 0.9280\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.3844 - acc: 0.9360\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.3630 - acc: 0.9430\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.3424 - acc: 0.9530\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.3217 - acc: 0.9580\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.3003 - acc: 0.9690\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.2823 - acc: 0.9680\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.2665 - acc: 0.9720\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.2507 - acc: 0.9780\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.2368 - acc: 0.9810\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.2229 - acc: 0.9830\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.2092 - acc: 0.9900\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.1971 - acc: 0.9910\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.1873 - acc: 0.9890\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.1787 - acc: 0.9890\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.1716 - acc: 0.9920\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.1644 - acc: 0.9920\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.1573 - acc: 0.9900\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 0.1509 - acc: 0.9930\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.1451 - acc: 0.9890\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1407 - acc: 0.9950\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1356 - acc: 0.9940\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.1312 - acc: 0.9970\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1274 - acc: 0.9930\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 59us/step - loss: 0.1238 - acc: 0.9960\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 96us/step - loss: 0.1202 - acc: 0.9930\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.1163 - acc: 0.9940\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 56us/step - loss: 0.1128 - acc: 0.9950\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 0.1096 - acc: 0.9930\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.1064 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.1043 - acc: 0.9910\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.1017 - acc: 0.9950\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0994 - acc: 0.9930\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 71us/step - loss: 0.0974 - acc: 0.9970\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 60us/step - loss: 0.0955 - acc: 0.9990: 0s - loss: 0.0970 - acc: 0.999\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 0.0937 - acc: 0.9940\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 65us/step - loss: 0.0919 - acc: 0.9950\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 0.0900 - acc: 0.9960\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 64us/step - loss: 0.0886 - acc: 0.9950\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 54us/step - loss: 0.0871 - acc: 0.9970\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 80us/step - loss: 0.0857 - acc: 0.9980\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 0.0844 - acc: 0.9990\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0831 - acc: 0.9950\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 45us/step - loss: 0.0818 - acc: 0.9960\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0807 - acc: 0.9950\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 50us/step - loss: 0.0794 - acc: 0.9980\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0782 - acc: 0.9940\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0771 - acc: 0.9970\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 58us/step - loss: 0.0760 - acc: 0.9980\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 48us/step - loss: 0.0750 - acc: 0.9930\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0743 - acc: 0.9970\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0734 - acc: 0.9970\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0726 - acc: 0.9960\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0375 - acc: 1.000 - 0s 36us/step - loss: 0.0719 - acc: 0.9960\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.0710 - acc: 0.9980\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0701 - acc: 0.9980\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0692 - acc: 0.9970\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0686 - acc: 0.9940\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0676 - acc: 0.9940\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0669 - acc: 0.9960\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0664 - acc: 0.9980\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0656 - acc: 0.9930\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0652 - acc: 0.9980\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0644 - acc: 0.9970\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0638 - acc: 0.9940\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0632 - acc: 0.9990\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0624 - acc: 0.9940\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 73us/step - loss: 0.0619 - acc: 0.9990\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0610 - acc: 0.9960\n",
      "Epoch 83/200\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 0.0610 - acc: 0.9940\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.0604 - acc: 0.9940\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0598 - acc: 0.9970\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 42us/step - loss: 0.0593 - acc: 0.9960\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0589 - acc: 0.9970\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 41us/step - loss: 0.0582 - acc: 0.9980\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0577 - acc: 0.9940\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0572 - acc: 0.9960\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0568 - acc: 0.9990\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0557 - acc: 0.9960\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 44us/step - loss: 0.0565 - acc: 0.9960\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 43us/step - loss: 0.0556 - acc: 0.9980\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0553 - acc: 0.9970\n",
      "Epoch 00095: early stopping\n"
     ]
    }
   ],
   "source": [
    "# build a sequential model, and train it with early stopping\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu', input_dim=1))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# early stop when training doesn't improve\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=3, verbose=1, mode='auto') # early stop if the loss doesn't get better in 3 epohcs\n",
    "callbacks_list = [earlystop]\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_history = model.fit(train_data, labels, epochs=200, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "input data: \n",
      "[[3.30555026e-01]\n",
      " [1.05609231e-01]\n",
      " [8.28824674e-01]\n",
      " [5.79748706e-01]\n",
      " [9.57009169e-01]\n",
      " [3.07118779e-01]\n",
      " [8.68787141e-02]\n",
      " [2.99640339e-01]\n",
      " [5.96251702e-01]\n",
      " [8.25703083e-04]\n",
      " [6.42678530e-01]\n",
      " [9.41017288e-01]\n",
      " [9.75168625e-01]\n",
      " [1.33063958e-01]\n",
      " [3.14145189e-01]\n",
      " [1.51850980e-01]\n",
      " [6.88660785e-02]\n",
      " [6.44946896e-01]\n",
      " [9.35299193e-02]\n",
      " [8.77202045e-02]]\n",
      "predictions: \n",
      "[[4.7538732e-03]\n",
      " [4.5872419e-04]\n",
      " [9.9997568e-01]\n",
      " [9.3356955e-01]\n",
      " [9.9999964e-01]\n",
      " [2.2486269e-03]\n",
      " [4.0635825e-04]\n",
      " [1.8138274e-03]\n",
      " [9.5975906e-01]\n",
      " [2.3282311e-04]\n",
      " [9.9062061e-01]\n",
      " [9.9999928e-01]\n",
      " [9.9999976e-01]\n",
      " [5.4790865e-04]\n",
      " [2.8149795e-03]\n",
      " [6.1873085e-04]\n",
      " [3.6164417e-04]\n",
      " [9.9127263e-01]\n",
      " [4.2423140e-04]\n",
      " [4.0857718e-04]]\n"
     ]
    }
   ],
   "source": [
    "# build more random sample data (100 examples) and test the predictions:\n",
    "data = np.random.random_sample((100, 1))\n",
    "predictions = model.predict(data, batch_size=32)\n",
    "print(\"-----------------------\")\n",
    "print(\"input data: \\n\" + str(data[:20]))\n",
    "print(\"predictions: \\n\" + str(predictions[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dc312e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# clearly draws the sigmoid at the 0.5 value\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data, predictions, color=\"red\")\n",
    "ax.set_xlabel('predictions')\n",
    "ax.set_ylabel('data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
